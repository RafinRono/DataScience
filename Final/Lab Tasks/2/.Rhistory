text <- gsub("\\baren't\\b", "are not", text)
text <- gsub("\\bwasn't\\b", "was not", text)
text <- gsub("\\bweren't\\b", "were not", text)
text <- gsub("\\b i've\\b", " I have", text)
text <- gsub("\\b we've\\b", " We have", text)
return(text)
}
data_no_contraction <- data_no_emojis %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
un_contracted <- expand_contractions_manual(x)
return(un_contracted)
})))
print(data_no_contraction)
data_corrected <- data_no_contraction %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
words <- unlist(strsplit(x, " "))
corrected_text <- sapply(words, function(word) {
word <- as.character(word)
misspelled_words <- hunspell(word)
if (length(misspelled_words[[1]]) > 0) {
suggestions <- hunspell_suggest(misspelled_words[[1]])
if (length(suggestions) > 0) {
return(suggestions[[1]][1])
} else {
return(word)
}
} else {
return(word)
}
})
paste(corrected_text, collapse = " ")
})))
print(data_corrected)
data_cleaned <- data_corrected %>%
mutate(
Snippet = Snippet %>%
tolower() %>%
gsub("<[^>]+>", "", .) %>%
gsub("[[:punct:]]+", "", .) %>%
gsub("[0-9]", " ", .) %>%
gsub("[^[:alnum:] ]", "", .) %>%
gsub("\\s+", " ", .) %>%
trimws()
)
data_cleaned <- data_cleaned %>%
mutate(
Headline = Headline %>%
tolower() %>%
gsub("<[^>]+>", "", .) %>%
gsub("[[:punct:]]+", "", .) %>%
gsub("[0-9]", " ", .) %>%
gsub("[^[:alnum:] ]", "", .) %>%
gsub("\\s+", " ", .) %>%
trimws()
)
print(data_cleaned)
data_tokenized <- data_cleaned %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
tokens <- tokenize_words(x)
tokens[[1]]
})))
print(data_tokenized)
ordinal_map <- c("first" = "one", "second" = "two", "third" = "three",
"fourth" = "four", "fifth" = "five", "sixth" = "six",
"seventh" = "seven", "eighth" = "eight", "ninth" = "nine",
"tenth" = "ten")
data_sampled <- data_tokenized %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
words <- unlist(strsplit(x, " "))
words <- sapply(words, function(word) {
if (word %in% names(ordinal_map)) {
return(ordinal_map[word])
} else {
return(word)
}
})
return(unname(words))
})))
print(data_sampled)
stop_words <- stopwords("en")
print(stop_words)
data_no_stopwords <- data_sampled %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
tokens_no_stopwords <- x[!x %in% stop_words]
return(tokens_no_stopwords)
})))
print(data_no_stopwords)
data_stemmed_lemmetated <- data_no_stopwords %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
lemmatized_text <- lemmatize_words(unlist(strsplit(x, " ")))
stemmed_text <- stemDocument(lemmatized_text)
return(stemmed_text)
})))
print(data_stemmed_lemmetated)
data_processed <- data_stemmed_lemmetated %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
return(paste(unlist(x), collapse = ", "))
})))
print(data_processed)
write.xlsx(data_processed, "data_processed_all_cols.xlsx")
data_stemmed_lemmetated <- data_no_stopwords %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
lemmatized_text <- lemmatize_words(unlist(strsplit(x, " ")))
#stemmed_text <- stemDocument(lemmatized_text)
return(lemmatized_text)
})))
print(data_stemmed_lemmetated)
data_stemmed_lemmetated <- data_no_stopwords %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
lemmatized_text <- lemmatize_words((strsplit(x, " ")))
#stemmed_text <- stemDocument(lemmatized_text)
return(lemmatized_text)
})))
print(data_stemmed_lemmetated)
print(data_no_stopwords)
data_stemmed_lemmetated <- data_no_stopwords %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
lemmatized_text <- lemmatize_words((strsplit(x, " ")))
#stemmed_text <- stemDocument(lemmatized_text)
return(lemmatized_text)
})))
print(data_stemmed_lemmetated)
data_stemmed_lemmetated <- data_no_stopwords %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
lemmatized_text <- lemmatize_words((x))
#stemmed_text <- stemDocument(lemmatized_text)
return(lemmatized_text)
})))
print(data_stemmed_lemmetated)
print(data_no_stopwords)
data_stemmed_lemmetated <- data_no_stopwords %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
lemmatized_text <- lemmatize_words(x)
stemmed_text <- stemDocument(lemmatized_text)
return(stemmed_text)
})))
print(data_stemmed_lemmetated)
data_processed <- data_stemmed_lemmetated %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
return(paste(unlist(x), collapse = ", "))
})))
print(data_processed)
write.xlsx(data_processed, "data_processed_all_cols.xlsx")
# Define the website URL
url <- "https://www.thedailystar.net/todays-news"
# Create a bow object to interact with the website
bow <- bow(url)
# Create a bow object to interact with the website
bow <- bow(url)
library(dplyr)
library(rvest)
library(polite)
library(tokenizers)
library(tm)
library(textstem)
library(stringr)
library(hunspell)
library(openxlsx)
# Define the website URL
url <- "https://www.thedailystar.net/todays-news"
# Create a bow object to interact with the website
bow <- bow(url)
View(bow)
rules<-bow$robotstxt
print(rules)
View(rules)
page <- read_html(url)
View(page)
# Extract headlines
headlines <- page %>%
html_nodes(".title a") %>%  # Adjust the CSS selector if necessary
html_text()
# Extract snippets (assumes there's a description element)
snippets <- page %>%
html_nodes("p.intro") %>%  # Replace with the correct CSS selector for snippets
html_text()
# Combine headlines and snippets into a data frame
data <- data.frame(Headline = headlines, Snippet = snippets)
View(data)
data_no_emojis <- data %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
emoji_pattern <- "[^\x01-\x7F]"
# Remove emojis
text_without_emojis <- str_remove_all(x, emoji_pattern)
return(text_without_emojis)  # Return the final cleaned tokens
})))
print(data_no_emojis)
# Custom function to expand contractions
expand_contractions_manual <- function(text) {
text <- gsub("\\bcan't\\b", "cannot", text)
text <- gsub("\\bwon't\\b", "will not", text)
text <- gsub("\\bshouldn't\\b", "should not", text)
text <- gsub("\\bdon't\\b", "do not", text)
text <- gsub("\\bdidn't\\b", "did not", text)
text <- gsub("\\bhasn't\\b", "has not", text)
text <- gsub("\\bhaven't\\b", "have not", text)
text <- gsub("\\b isn't\\b", " is not", text)
text <- gsub("\\baren't\\b", "are not", text)
text <- gsub("\\bwasn't\\b", "was not", text)
text <- gsub("\\bweren't\\b", "were not", text)
text <- gsub("\\b i've\\b", " I have", text)
text <- gsub("\\b we've\\b", " We have", text)
return(text)
}
data_no_contraction <- data_no_emojis %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
un_contracted <- expand_contractions_manual(x)
return(un_contracted)  # Return the final cleaned tokens
})))
print(data_no_contraction)
# Apply mutation to the 'Snippet' column to check and correct spelling
data_corrected <- data_no_contraction %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
# Split the string into individual words
words <- unlist(strsplit(x, " "))  # Ensure words is a character vector
# Correct the words if there are any misspellings
corrected_text <- sapply(words, function(word) {
# Ensure word is a character before passing to hunspell
word <- as.character(word)
# Check for misspelled words using hunspell
misspelled_words <- hunspell(word)
# If there are misspelled words, get suggestions
if (length(misspelled_words[[1]]) > 0) {
suggestions <- hunspell_suggest(misspelled_words[[1]])
# If suggestions exist, return the first suggestion
if (length(suggestions) > 0) {
return(suggestions[[1]][1])  # First suggestion
} else {
return(word)  # No suggestions, return the original word
}
} else {
return(word)  # If the word is correctly spelled, return as is
}
})
# Combine the corrected words back into a sentence
paste(corrected_text, collapse = " ")
})))
data_cleaned <- data_corrected %>%
mutate(
Snippet = Snippet %>%
tolower() %>%                             # Convert to lowercase
gsub("<[^>]+>", "", .) %>%                # Remove HTML tags
gsub("[[:punct:]]+", "", .) %>%           # Replace punctuation with space (one or more punctuation characters)
gsub("[0-9]", " ", .) %>%                  # Remove numbers
gsub("[^[:alnum:] ]", "", .) %>%        # Remove special characters (except spaces)
gsub("\\s+", " ", .) %>%                  # Replace multiple spaces with a single space
trimws()                                  # Remove leading/trailing spaces
)
data_cleaned <- data_cleaned %>%
mutate(
Headline = Headline %>%
tolower() %>%                             # Convert to lowercase
gsub("<[^>]+>", "", .) %>%                # Remove HTML tags
gsub("[[:punct:]]+", "", .) %>%           # Replace punctuation with space (one or more punctuation characters)
gsub("[0-9]", " ", .) %>%                  # Remove numbers
gsub("[^[:alnum:] ]", "", .) %>%        # Remove special characters (except spaces)
gsub("\\s+", " ", .) %>%                  # Replace multiple spaces with a single space
trimws()                                  # Remove leading/trailing spaces
)
print(data_cleaned)
View(data_cleaned)
# Tokenize the 'Snippet' column into a list of words
data_tokenized <- data_cleaned %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
tokens <- tokenize_words(x)  # Tokenize the text into words
tokens[[1]]  # Return the first (and only) element, which is the list of tokens
})))
# Print the cleaned data
print(data_tokenized)
View(data_tokenized)
data_sampled <- data_tokenized %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
#words <- unlist(strsplit(x, " "))  # Split into words
# Replace ordinal words with their corresponding numeric values
words <- sapply(words, function(x) {
if (word %in% names(ordinal_map)) {
return(ordinal_map[word])  # Replace with corresponding number word
} else {
return(word)  # Keep the word as it is
}
})
# Return the words as a character vector, unname() ensures no key-value pair
#return(paste(unname(words), collapse = " "))  # Recombine words back into a sentence
return(unname(words))
})))
data_sampled <- data_tokenized %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
words <- unlist(strsplit(x, " "))  # Split into words
# Replace ordinal words with their corresponding numeric values
words <- sapply(words, function(word) {
if (word %in% names(ordinal_map)) {
return(ordinal_map[word])  # Replace with corresponding number word
} else {
return(word)  # Keep the word as it is
}
})
# Return the words as a character vector, unname() ensures no key-value pair
#return(paste(unname(words), collapse = " "))  # Recombine words back into a sentence
return(unname(words))
})))
ordinal_map <- c("first" = "one", "second" = "two", "third" = "three",
"fourth" = "four", "fifth" = "five", "sixth" = "six",
"seventh" = "seven", "eighth" = "eight", "ninth" = "nine",
"tenth" = "ten")
data_sampled <- data_tokenized %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
#words <- unlist(strsplit(x, " "))  # Split into words
# Replace ordinal words with their corresponding numeric values
words <- sapply(words, function(x) {
if (word %in% names(ordinal_map)) {
return(ordinal_map[word])  # Replace with corresponding number word
} else {
return(word)  # Keep the word as it is
}
})
# Return the words as a character vector, unname() ensures no key-value pair
#return(paste(unname(words), collapse = " "))  # Recombine words back into a sentence
return(unname(words))
})))
data_sampled <- data_tokenized %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
words <- unlist(strsplit(x, " "))  # Split into words
# Replace ordinal words with their corresponding numeric values
words <- sapply(words, function(word) {
if (word %in% names(ordinal_map)) {
return(ordinal_map[word])  # Replace with corresponding number word
} else {
return(word)  # Keep the word as it is
}
})
# Return the words as a character vector, unname() ensures no key-value pair
#return(paste(unname(words), collapse = " "))  # Recombine words back into a sentence
return(unname(words))
})))
print(data_sampled)
stop_words <- stopwords("en")
print(stop_words)
data_no_stopwords <- data_sampled %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
tokens_no_stopwords <- x[!x %in% stop_words]  # Remove stop words
return(tokens_no_stopwords)  # Return the cleaned tokens without stop words
})))
# Print the cleaned data
print(data_no_stopwords)
data_stemmed_lemmetated <- data_no_stopwords %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
lemmatized_text <- lemmatize_words(x)
stemmed_text <- stem_words(lemmatized_text)
return(stemmed_text)
})))
print(data_stemmed_lemmetated)
print(data_stemmed_lemmetated)
View(data_stemmed_lemmetated)
data_processed <- data_stemmed_lemmetated %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
return(paste(unlist(x), collapse = ", "))  # Convert the list to a string
})))
print(data_processed)
View(data_processed)
library(dplyr)
library(ggplot2)
library(reshape2)
data <- read.csv("london_houses.csv", header = TRUE)
rows_with_na <- sum(apply(data, 1, function(row) any(is.na(row))))
rows_with_na
numeric_cols <- sapply(data, is.numeric)
for (col in names(data)[numeric_cols]) {
hist_data <- hist(data[[col]], plot = FALSE)
plot(hist_data$mids, hist_data$density, type = "l",
main = paste("Line Histogram of", col),
xlab = col, ylab = "Density", col = "darkgreen", lwd = 2)
}
numeric_cols <- sapply(data, is.numeric)
for (col in names(data)[numeric_cols]) {
hist(data[[col]], main = paste("Histogram of", col), xlab = col, col = "lightgreen")
}
numeric_cols <- sapply(data, is.numeric)
for (col in names(data)[numeric_cols]) {
boxplot(data[[col]],
main = paste("Box Plot of", col),
ylab = col, col = "orange")
}
categorical_encoded <- data
Price <- cut(categorical_encoded$Price, breaks = 3)
levels(Price)
levels(Price) <- c("Low", "Medium", "High")
categorical_encoded$Price_labels <- Price
table(categorical_encoded$Price_labels)
color_mapping <- c("Low" = "blue", "Medium" = "green", "High" = "red")
colors <- color_mapping[categorical_encoded$Price_labels]
pairs(categorical_encoded[, sapply(categorical_encoded, is.numeric)],
col = colors,
pch = 19,
main = "Scatterplot Matrix")
numeric_columns <- names(categorical_encoded)[sapply(categorical_encoded, is.numeric)]
for (col in numeric_columns) {
if (col == "Price") next
plot <- ggplot(categorical_encoded, aes_string(x = "Price_labels", y = col)) +
geom_violin(fill = "lightgreen", color = "black") +
geom_boxplot(width = 0.1, outlier.color = "red") +
labs(title = paste("Violin Plot of", col, "by Price Labels"),
x = "Price Labels",
y = col) +
theme_minimal()
print(plot)
}
numeric_cols <- names(categorical_encoded)[sapply(categorical_encoded, is.numeric)]
for (i in 1:(length(numeric_cols) - 1)) {
for (j in (i + 1):length(numeric_cols)) {
combo <- c(numeric_cols[i], numeric_cols[j])
plot_data <- data.frame(
x = categorical_encoded[[combo[1]]][1:30],
y = categorical_encoded[[combo[2]]][1:30]
)
plot <- ggplot(plot_data, aes(x = x, y = y)) +
geom_line(size = 1) +
geom_point(size = 3) +
labs(x = combo[1], y = combo[2], title = paste("Line Graph for", paste(combo, collapse = " vs. "))) +
theme_minimal()
print(plot)
}
}
library(dplyr)
library(ggplot2)
library(reshape2)
data <- read.csv("london_houses.csv", header = TRUE)
rows_with_na <- sum(apply(data, 1, function(row) any(is.na(row))))
setwd("J:/R/Workspace/Lab Tasks/2")
data <- read.csv("london_houses.csv", header = TRUE)
rows_with_na <- sum(apply(data, 1, function(row) any(is.na(row))))
rows_with_na
numeric_cols <- sapply(data, is.numeric)
for (col in names(data)[numeric_cols]) {
hist_data <- hist(data[[col]], plot = FALSE)
plot(hist_data$mids, hist_data$density, type = "l",
main = paste("Line Histogram of", col),
xlab = col, ylab = "Density", col = "darkgreen", lwd = 2)
}
numeric_cols <- sapply(data, is.numeric)
for (col in names(data)[numeric_cols]) {
hist(data[[col]], main = paste("Histogram of", col), xlab = col, col = "lightgreen")
}
numeric_cols <- sapply(data, is.numeric)
for (col in names(data)[numeric_cols]) {
boxplot(data[[col]],
main = paste("Box Plot of", col),
ylab = col, col = "orange")
}
categorical_encoded <- data
Price <- cut(categorical_encoded$Price, breaks = 3)
levels(Price)
levels(Price) <- c("Low", "Medium", "High")
categorical_encoded$Price_labels <- Price
table(categorical_encoded$Price_labels)
color_mapping <- c("Low" = "blue", "Medium" = "green", "High" = "red")
colors <- color_mapping[categorical_encoded$Price_labels]
pairs(categorical_encoded[, sapply(categorical_encoded, is.numeric)],
col = colors,
pch = 19,
main = "Scatterplot Matrix")
numeric_columns <- names(categorical_encoded)[sapply(categorical_encoded, is.numeric)]
for (col in numeric_columns) {
if (col == "Price") next
plot <- ggplot(categorical_encoded, aes_string(x = "Price_labels", y = col)) +
geom_violin(fill = "lightgreen", color = "black") +
geom_boxplot(width = 0.1, outlier.color = "red") +
labs(title = paste("Violin Plot of", col, "by Price Labels"),
x = "Price Labels",
y = col) +
theme_minimal()
print(plot)
}
numeric_cols <- names(categorical_encoded)[sapply(categorical_encoded, is.numeric)]
for (i in 1:(length(numeric_cols) - 1)) {
for (j in (i + 1):length(numeric_cols)) {
combo <- c(numeric_cols[i], numeric_cols[j])
plot_data <- data.frame(
x = categorical_encoded[[combo[1]]][1:30],
y = categorical_encoded[[combo[2]]][1:30]
)
plot <- ggplot(plot_data, aes(x = x, y = y)) +
geom_line(size = 1) +
geom_point(size = 3) +
labs(x = combo[1], y = combo[2], title = paste("Line Graph for", paste(combo, collapse = " vs. "))) +
theme_minimal()
print(plot)
}
}
