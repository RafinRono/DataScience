# Replace ordinal words with their corresponding numeric values
words <- sapply(words, function(word) {
if (word %in% names(ordinal_map)) {
return(ordinal_map[word])  # Replace with corresponding number word
} else {
return(word)  # Keep the word as it is
}
})
# Return the words as a character vector, unname() ensures no key-value pair
#return(paste(unname(words), collapse = " "))  # Recombine words back into a sentence
#return(unname(words))
})))
print(data_sampled)
data_sampled <- data_tokenized %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
words <- unlist(strsplit(x, " "))  # Split into words
# Replace ordinal words with their corresponding numeric values
words <- sapply(words, function(word) {
if (word %in% names(ordinal_map)) {
return(ordinal_map[word])  # Replace with corresponding number word
} else {
return(word)  # Keep the word as it is
}
})
# Return the words as a character vector, unname() ensures no key-value pair
#return(paste(unname(words), collapse = " "))  # Recombine words back into a sentence
return(unname(words))
})))
print(data_sampled)
data_stemmed_lemmetated <- data_no_stopwords %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
stemmed_text <- stemDocument(x)
lemmatized_text <- lemmatize_words(unlist(strsplit(stemmed_text, " ")))
return(lemmatized_text)
})))
print(data_stemmed_lemmetated)
data_processed <- data_stemmed_lemmetated %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
return(paste(unlist(x), collapse = ", "))
})))
print(data_processed)
data_stemmed_lemmetated <- data_no_stopwords %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
lemmatized_text <- lemmatize_words(unlist(strsplit(x, " ")))
stemmed_text <- stemDocument(lemmatized_text)
return(stemmed_text)
})))
print(data_stemmed_lemmetated)
library(dplyr)
library(rvest)
library(polite)
library(tokenizers)
library(tm)
library(textstem)
library(stringr)
library(hunspell)
library(openxlsx)
url <- "https://www.thedailystar.net/todays-news"
bow <- bow(url)
print(bow)
rules<-bow$robotstxt
print(rules)
page <- read_html(url)
headlines <- page %>%
html_nodes(".title a") %>%
html_text()
snippets <- page %>%
html_nodes("p.intro") %>%
html_text()
data <- data.frame(Headline = headlines, Snippet = snippets)
write.xlsx(data, "data_original.xlsx")
print(headlines)
print(snippets)
print(data)
data_no_emojis <- data %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
emoji_pattern <- "[^\x01-\x7F]"
text_without_emojis <- str_remove_all(x, emoji_pattern)
return(text_without_emojis)
})))
print(data_no_emojis)
expand_contractions_manual <- function(text) {
text <- gsub("\\bcan't\\b", "cannot", text)
text <- gsub("\\bwon't\\b", "will not", text)
text <- gsub("\\bshouldn't\\b", "should not", text)
text <- gsub("\\bdon't\\b", "do not", text)
text <- gsub("\\bdidn't\\b", "did not", text)
text <- gsub("\\bhasn't\\b", "has not", text)
text <- gsub("\\bhaven't\\b", "have not", text)
text <- gsub("\\b isn't\\b", " is not", text)
text <- gsub("\\baren't\\b", "are not", text)
text <- gsub("\\bwasn't\\b", "was not", text)
text <- gsub("\\bweren't\\b", "were not", text)
text <- gsub("\\b i've\\b", " I have", text)
text <- gsub("\\b we've\\b", " We have", text)
return(text)
}
data_no_contraction <- data_no_emojis %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
un_contracted <- expand_contractions_manual(x)
return(un_contracted)
})))
print(data_no_contraction)
data_corrected <- data_no_contraction %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
words <- unlist(strsplit(x, " "))
corrected_text <- sapply(words, function(word) {
word <- as.character(word)
misspelled_words <- hunspell(word)
if (length(misspelled_words[[1]]) > 0) {
suggestions <- hunspell_suggest(misspelled_words[[1]])
if (length(suggestions) > 0) {
return(suggestions[[1]][1])
} else {
return(word)
}
} else {
return(word)
}
})
paste(corrected_text, collapse = " ")
})))
print(data_corrected)
data_cleaned <- data_corrected %>%
mutate(
Snippet = Snippet %>%
tolower() %>%
gsub("<[^>]+>", "", .) %>%
gsub("[[:punct:]]+", "", .) %>%
gsub("[0-9]", " ", .) %>%
gsub("[^[:alnum:] ]", "", .) %>%
gsub("\\s+", " ", .) %>%
trimws()
)
data_cleaned <- data_cleaned %>%
mutate(
Headline = Headline %>%
tolower() %>%
gsub("<[^>]+>", "", .) %>%
gsub("[[:punct:]]+", "", .) %>%
gsub("[0-9]", " ", .) %>%
gsub("[^[:alnum:] ]", "", .) %>%
gsub("\\s+", " ", .) %>%
trimws()
)
print(data_cleaned)
data_tokenized <- data_cleaned %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
tokens <- tokenize_words(x)
tokens[[1]]
})))
print(data_tokenized)
ordinal_map <- c("first" = "one", "second" = "two", "third" = "three",
"fourth" = "four", "fifth" = "five", "sixth" = "six",
"seventh" = "seven", "eighth" = "eight", "ninth" = "nine",
"tenth" = "ten")
data_sampled <- data_tokenized %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
words <- unlist(strsplit(x, " "))
words <- sapply(words, function(word) {
if (word %in% names(ordinal_map)) {
return(ordinal_map[word])
} else {
return(word)
}
})
return(unname(words))
})))
print(data_sampled)
stop_words <- stopwords("en")
print(stop_words)
data_no_stopwords <- data_sampled %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
tokens_no_stopwords <- x[!x %in% stop_words]
return(tokens_no_stopwords)
})))
print(data_no_stopwords)
data_stemmed_lemmetated <- data_no_stopwords %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
lemmatized_text <- lemmatize_words(unlist(strsplit(x, " ")))
stemmed_text <- stemDocument(lemmatized_text)
return(stemmed_text)
})))
print(data_stemmed_lemmetated)
data_processed <- data_stemmed_lemmetated %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
return(paste(unlist(x), collapse = ", "))
})))
print(data_processed)
write.xlsx(data_processed, "data_processed_all_cols.xlsx")
data_stemmed_lemmetated <- data_no_stopwords %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
lemmatized_text <- lemmatize_words(unlist(strsplit(x, " ")))
#stemmed_text <- stemDocument(lemmatized_text)
return(lemmatized_text)
})))
print(data_stemmed_lemmetated)
data_stemmed_lemmetated <- data_no_stopwords %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
lemmatized_text <- lemmatize_words((strsplit(x, " ")))
#stemmed_text <- stemDocument(lemmatized_text)
return(lemmatized_text)
})))
print(data_stemmed_lemmetated)
print(data_no_stopwords)
data_stemmed_lemmetated <- data_no_stopwords %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
lemmatized_text <- lemmatize_words((strsplit(x, " ")))
#stemmed_text <- stemDocument(lemmatized_text)
return(lemmatized_text)
})))
print(data_stemmed_lemmetated)
data_stemmed_lemmetated <- data_no_stopwords %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
lemmatized_text <- lemmatize_words((x))
#stemmed_text <- stemDocument(lemmatized_text)
return(lemmatized_text)
})))
print(data_stemmed_lemmetated)
print(data_no_stopwords)
data_stemmed_lemmetated <- data_no_stopwords %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
lemmatized_text <- lemmatize_words(x)
stemmed_text <- stemDocument(lemmatized_text)
return(stemmed_text)
})))
print(data_stemmed_lemmetated)
data_processed <- data_stemmed_lemmetated %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
return(paste(unlist(x), collapse = ", "))
})))
print(data_processed)
write.xlsx(data_processed, "data_processed_all_cols.xlsx")
# Define the website URL
url <- "https://www.thedailystar.net/todays-news"
# Create a bow object to interact with the website
bow <- bow(url)
# Create a bow object to interact with the website
bow <- bow(url)
library(dplyr)
library(rvest)
library(polite)
library(tokenizers)
library(tm)
library(textstem)
library(stringr)
library(hunspell)
library(openxlsx)
# Define the website URL
url <- "https://www.thedailystar.net/todays-news"
# Create a bow object to interact with the website
bow <- bow(url)
View(bow)
rules<-bow$robotstxt
print(rules)
View(rules)
page <- read_html(url)
View(page)
# Extract headlines
headlines <- page %>%
html_nodes(".title a") %>%  # Adjust the CSS selector if necessary
html_text()
# Extract snippets (assumes there's a description element)
snippets <- page %>%
html_nodes("p.intro") %>%  # Replace with the correct CSS selector for snippets
html_text()
# Combine headlines and snippets into a data frame
data <- data.frame(Headline = headlines, Snippet = snippets)
View(data)
data_no_emojis <- data %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
emoji_pattern <- "[^\x01-\x7F]"
# Remove emojis
text_without_emojis <- str_remove_all(x, emoji_pattern)
return(text_without_emojis)  # Return the final cleaned tokens
})))
print(data_no_emojis)
# Custom function to expand contractions
expand_contractions_manual <- function(text) {
text <- gsub("\\bcan't\\b", "cannot", text)
text <- gsub("\\bwon't\\b", "will not", text)
text <- gsub("\\bshouldn't\\b", "should not", text)
text <- gsub("\\bdon't\\b", "do not", text)
text <- gsub("\\bdidn't\\b", "did not", text)
text <- gsub("\\bhasn't\\b", "has not", text)
text <- gsub("\\bhaven't\\b", "have not", text)
text <- gsub("\\b isn't\\b", " is not", text)
text <- gsub("\\baren't\\b", "are not", text)
text <- gsub("\\bwasn't\\b", "was not", text)
text <- gsub("\\bweren't\\b", "were not", text)
text <- gsub("\\b i've\\b", " I have", text)
text <- gsub("\\b we've\\b", " We have", text)
return(text)
}
data_no_contraction <- data_no_emojis %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
un_contracted <- expand_contractions_manual(x)
return(un_contracted)  # Return the final cleaned tokens
})))
print(data_no_contraction)
# Apply mutation to the 'Snippet' column to check and correct spelling
data_corrected <- data_no_contraction %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
# Split the string into individual words
words <- unlist(strsplit(x, " "))  # Ensure words is a character vector
# Correct the words if there are any misspellings
corrected_text <- sapply(words, function(word) {
# Ensure word is a character before passing to hunspell
word <- as.character(word)
# Check for misspelled words using hunspell
misspelled_words <- hunspell(word)
# If there are misspelled words, get suggestions
if (length(misspelled_words[[1]]) > 0) {
suggestions <- hunspell_suggest(misspelled_words[[1]])
# If suggestions exist, return the first suggestion
if (length(suggestions) > 0) {
return(suggestions[[1]][1])  # First suggestion
} else {
return(word)  # No suggestions, return the original word
}
} else {
return(word)  # If the word is correctly spelled, return as is
}
})
# Combine the corrected words back into a sentence
paste(corrected_text, collapse = " ")
})))
data_cleaned <- data_corrected %>%
mutate(
Snippet = Snippet %>%
tolower() %>%                             # Convert to lowercase
gsub("<[^>]+>", "", .) %>%                # Remove HTML tags
gsub("[[:punct:]]+", "", .) %>%           # Replace punctuation with space (one or more punctuation characters)
gsub("[0-9]", " ", .) %>%                  # Remove numbers
gsub("[^[:alnum:] ]", "", .) %>%        # Remove special characters (except spaces)
gsub("\\s+", " ", .) %>%                  # Replace multiple spaces with a single space
trimws()                                  # Remove leading/trailing spaces
)
data_cleaned <- data_cleaned %>%
mutate(
Headline = Headline %>%
tolower() %>%                             # Convert to lowercase
gsub("<[^>]+>", "", .) %>%                # Remove HTML tags
gsub("[[:punct:]]+", "", .) %>%           # Replace punctuation with space (one or more punctuation characters)
gsub("[0-9]", " ", .) %>%                  # Remove numbers
gsub("[^[:alnum:] ]", "", .) %>%        # Remove special characters (except spaces)
gsub("\\s+", " ", .) %>%                  # Replace multiple spaces with a single space
trimws()                                  # Remove leading/trailing spaces
)
print(data_cleaned)
View(data_cleaned)
# Tokenize the 'Snippet' column into a list of words
data_tokenized <- data_cleaned %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
tokens <- tokenize_words(x)  # Tokenize the text into words
tokens[[1]]  # Return the first (and only) element, which is the list of tokens
})))
# Print the cleaned data
print(data_tokenized)
View(data_tokenized)
data_sampled <- data_tokenized %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
#words <- unlist(strsplit(x, " "))  # Split into words
# Replace ordinal words with their corresponding numeric values
words <- sapply(words, function(x) {
if (word %in% names(ordinal_map)) {
return(ordinal_map[word])  # Replace with corresponding number word
} else {
return(word)  # Keep the word as it is
}
})
# Return the words as a character vector, unname() ensures no key-value pair
#return(paste(unname(words), collapse = " "))  # Recombine words back into a sentence
return(unname(words))
})))
data_sampled <- data_tokenized %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
words <- unlist(strsplit(x, " "))  # Split into words
# Replace ordinal words with their corresponding numeric values
words <- sapply(words, function(word) {
if (word %in% names(ordinal_map)) {
return(ordinal_map[word])  # Replace with corresponding number word
} else {
return(word)  # Keep the word as it is
}
})
# Return the words as a character vector, unname() ensures no key-value pair
#return(paste(unname(words), collapse = " "))  # Recombine words back into a sentence
return(unname(words))
})))
ordinal_map <- c("first" = "one", "second" = "two", "third" = "three",
"fourth" = "four", "fifth" = "five", "sixth" = "six",
"seventh" = "seven", "eighth" = "eight", "ninth" = "nine",
"tenth" = "ten")
data_sampled <- data_tokenized %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
#words <- unlist(strsplit(x, " "))  # Split into words
# Replace ordinal words with their corresponding numeric values
words <- sapply(words, function(x) {
if (word %in% names(ordinal_map)) {
return(ordinal_map[word])  # Replace with corresponding number word
} else {
return(word)  # Keep the word as it is
}
})
# Return the words as a character vector, unname() ensures no key-value pair
#return(paste(unname(words), collapse = " "))  # Recombine words back into a sentence
return(unname(words))
})))
data_sampled <- data_tokenized %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
words <- unlist(strsplit(x, " "))  # Split into words
# Replace ordinal words with their corresponding numeric values
words <- sapply(words, function(word) {
if (word %in% names(ordinal_map)) {
return(ordinal_map[word])  # Replace with corresponding number word
} else {
return(word)  # Keep the word as it is
}
})
# Return the words as a character vector, unname() ensures no key-value pair
#return(paste(unname(words), collapse = " "))  # Recombine words back into a sentence
return(unname(words))
})))
print(data_sampled)
stop_words <- stopwords("en")
print(stop_words)
data_no_stopwords <- data_sampled %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
tokens_no_stopwords <- x[!x %in% stop_words]  # Remove stop words
return(tokens_no_stopwords)  # Return the cleaned tokens without stop words
})))
# Print the cleaned data
print(data_no_stopwords)
data_stemmed_lemmetated <- data_no_stopwords %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
lemmatized_text <- lemmatize_words(x)
stemmed_text <- stem_words(lemmatized_text)
return(stemmed_text)
})))
print(data_stemmed_lemmetated)
print(data_stemmed_lemmetated)
View(data_stemmed_lemmetated)
data_processed <- data_stemmed_lemmetated %>%
mutate(across(c(Snippet, Headline),
~ sapply(., function(x) {
return(paste(unlist(x), collapse = ", "))  # Convert the list to a string
})))
print(data_processed)
View(data_processed)
setwd("J:/R/Workspace/Assignment")
library(Kendall)
library(dplyr)
data <- read.csv("london_houses.csv", header = TRUE)
data <- read.csv("london_houses.csv", header = TRUE)
rows_with_na <- sum(apply(data, 1, function(row) any(is.na(row))))
rows_with_na
pearson_results <- list()
spearman_results <- list()
anova_results <- list()
kendall_results <- list()
for (col in names(data)) {
if (col == "Price") next
if (is.numeric(data[[col]])) {
pearson_corr <- cor(data$Price, data[[col]], method = "pearson", use = "complete.obs")
pearson_results[[col]] <- pearson_corr
spearman_corr <- cor(data$Price, data[[col]], method = "spearman", use = "complete.obs")
spearman_results[[col]] <- spearman_corr
}
if (is.factor(data[[col]]) || is.character(data[[col]])) {
fit <- lm(Price ~ as.factor(data[[col]]), data = data)
anova_result <- anova(fit)
anova_results[[col]] <- anova_result
kendall_corr <- Kendall(data$Price, as.factor(data[[col]]))
kendall_results[[col]] <- kendall_corr
}
}
print("Pearson's Correlation Coefficients:")
print(pearson_results)
print("Spearman's Correlation Coefficients:")
print(spearman_results)
print("ANOVA Results for Categorical Columns:")
print(anova_results)
print(kendall_results)
